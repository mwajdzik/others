{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x11702d080>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://10.136.180.136:4041\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.4.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>pyspark-shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 27
    }
   ],
   "source": [
    "import os.path\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n|cpu_utilization|         event_date|free_memory|server_id|session_count|\n+---------------+-------------------+-----------+---------+-------------+\n|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|\n|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|\n|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|\n|           0.41|03/05/2019 08:31:14|       0.58|      100|           48|\n|           0.57|03/05/2019 08:36:14|       0.35|      100|           58|\n|           0.41|03/05/2019 08:41:14|        0.4|      100|           58|\n|           0.53|03/05/2019 08:46:14|       0.35|      100|           62|\n|           0.51|03/05/2019 08:51:14|        0.6|      100|           45|\n+---------------+-------------------+-----------+---------+-------------+\nonly showing top 10 rows\n\nroot\n |-- cpu_utilization: double (nullable = true)\n |-- event_date: string (nullable = true)\n |-- free_memory: double (nullable = true)\n |-- server_id: long (nullable = true)\n |-- session_count: long (nullable = true)\n\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "500000"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 2
    }
   ],
   "source": [
    "folder_path = '/Users/sg0218817/Downloads/Ex_Files_Spark_SQL_DataFrames/Exercise Files/Data'\n",
    "df = spark.read.format('json')\\\n",
    "    .load(os.path.join(folder_path, 'utilization'))\n",
    "\n",
    "df.show(10)\n",
    "df.printSchema()\n",
    "df.count()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "+---+----+\n| id| cpu|\n+---+----+\n|100|0.57|\n|100|0.56|\n|100|0.57|\n|100|0.57|\n|100|0.53|\n+---+----+\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "df.createOrReplaceTempView('utilization')\n",
    "\n",
    "df_sql = spark.sql('SELECT server_id AS id, cpu_utilization AS cpu '\n",
    "                   'FROM utilization '\n",
    "                   'WHERE cpu_utilization > 0.5 '\n",
    "                   'LIMIT 5')\n",
    "df_sql.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "+---+-------+\n| id|avg_cpu|\n+---+-------+\n|101| 0.7986|\n|113| 0.7833|\n|145| 0.7823|\n|103| 0.7614|\n|102| 0.7584|\n|133| 0.7563|\n|108| 0.7476|\n|149| 0.7448|\n|137| 0.7408|\n|148| 0.7394|\n|123| 0.7314|\n|118| 0.7287|\n|112| 0.7154|\n|139| 0.7154|\n|104| 0.7109|\n|142| 0.7033|\n|121|  0.702|\n|146| 0.6997|\n|126| 0.6785|\n|144| 0.6739|\n+---+-------+\nonly showing top 20 rows\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "df_sql = spark.sql('SELECT server_id AS id, ROUND(AVG(cpu_utilization), 4) AS avg_cpu '\n",
    "                   'FROM utilization '\n",
    "                   'GROUP BY server_id '\n",
    "                   'ORDER BY avg_cpu DESC')\n",
    "df_sql.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "+-----------+------------------+\n|       name|           avg_cpu|\n+-----------+------------------+\n|Server #101|0.7985559999999872|\n|Server #113|0.7833319999999914|\n|Server #145|0.7823149999999972|\n|Server #103|0.7614389999999969|\n|Server #102|0.7583949999999904|\n|Server #133|0.7562959999999967|\n|Server #108| 0.747636000000003|\n|Server #149|0.7447700000000035|\n|Server #137|0.7407720000000032|\n|Server #148|0.7393840000000045|\n|Server #123|0.7314100000000078|\n|Server #118|0.7286740000000046|\n|Server #112|0.7153870000000067|\n|Server #139|0.7153610000000032|\n|Server #104|0.7108530000000015|\n|Server #142|0.7032940000000032|\n|Server #121|0.7019790000000099|\n|Server #146| 0.699687000000013|\n|Server #126|0.6784700000000029|\n|Server #144|0.6738750000000022|\n+-----------+------------------+\nonly showing top 20 rows\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "df_server = spark.read.format('csv')\\\n",
    "    .option('header', 'true')\\\n",
    "    .load(os.path.join(folder_path, 'server_names.csv'))\n",
    "\n",
    "df_server.createOrReplaceTempView('server_names')\n",
    "\n",
    "df_sql = spark.sql('SELECT sn.server_name as `name`, AVG(u.cpu_utilization) as `avg_cpu` '\n",
    "                   'FROM utilization u '\n",
    "                   'INNER JOIN server_names sn ON u.server_id = sn.server_id '\n",
    "                   'GROUP BY sn.server_name '\n",
    "                   'ORDER BY avg_cpu DESC')\n",
    "df_sql.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}