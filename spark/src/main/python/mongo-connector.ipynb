{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "\nfrom pyspark import SparkConf\nfrom pyspark.sql import SparkSession\n\nconf \u003d SparkConf()\nconf.set(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:2.4.0\")\nconf.set(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/test.coll\")\nconf.set(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/test.coll\")\n\nspark \u003d SparkSession.builder \\\n    .appName(\"test-mongo\") \\\n    .master(\u0027local[*]\u0027) \\\n    .config(conf\u003dconf) \\\n    .getOrCreate()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "+-------------+----+\n|         name| age|\n+-------------+----+\n|Bilbo Baggins|  50|\n|      Gandalf|1000|\n|       Thorin| 195|\n|        Balin| 178|\n|         Kili|  77|\n|       Dwalin| 169|\n|          Oin| 167|\n|        Gloin| 158|\n|         Fili|  82|\n|       Bombur|null|\n+-------------+----+\n\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "\npeople \u003d spark.createDataFrame([\n    (\"Bilbo Baggins\",  50), (\"Gandalf\", 1000), (\"Thorin\", 195),\n    (\"Balin\", 178), (\"Kili\", 77), (\"Dwalin\", 169), (\"Oin\", 167),\n    (\"Gloin\", 158), (\"Fili\", 82), (\"Bombur\", None)], [\"name\", \"age\"])\n\npeople.show()\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "WRITE/READ\nroot\n |-- name: string (nullable \u003d true)\n |-- age: long (nullable \u003d true)\n\n+--------------------+----+-------------+\n|                 _id| age|         name|\n+--------------------+----+-------------+\n|[5cc6ac7d78312462...|  82|         Fili|\n|[5cc6ac7d78312462...|null|       Bombur|\n|[5cc6ac7d78312462...|  50|Bilbo Baggins|\n|[5cc6ac7d78312462...| 195|       Thorin|\n|[5cc6ac7d78312462...| 158|        Gloin|\n|[5cc6ac7d78312462...|1000|      Gandalf|\n|[5cc6ac7d78312462...| 167|          Oin|\n|[5cc6ac7d78312462...| 178|        Balin|\n|[5cc6ac7d78312462...|  77|         Kili|\n|[5cc6ac7d78312462...| 169|       Dwalin|\n|[5cc6bc4c78312462...| 167|          Oin|\n|[5cc6bc4c78312462...|1000|      Gandalf|\n|[5cc6bc4c78312462...| 169|       Dwalin|\n|[5cc6bc4c78312462...| 195|       Thorin|\n|[5cc6bc4c78312462...| 178|        Balin|\n|[5cc6bc4c78312462...|  77|         Kili|\n|[5cc6bc4c78312462...|  50|Bilbo Baggins|\n|[5cc6bc4c78312462...|  82|         Fili|\n|[5cc6bc4c78312462...|null|       Bombur|\n|[5cc6bc4c78312462...| 158|        Gloin|\n+--------------------+----+-------------+\n\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "\nprint(\"WRITE/READ\")\n\npeople.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"append\")\\\n    .option(\"database\", \"people\") \\\n    .option(\"collection\", \"contacts\") \\\n    .save()\n\npeople.printSchema()\n\ndf \u003d spark.read.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n    .load()\n\ndf.show()\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "CHANGE COLLECTION\n+--------------------+----+-------------+\n|                 _id| age|         name|\n+--------------------+----+-------------+\n|[5cc6bc9578312462...|  82|         Fili|\n|[5cc6bc9578312462...|null|       Bombur|\n|[5cc6bc9578312462...| 167|          Oin|\n|[5cc6bc9578312462...|  50|Bilbo Baggins|\n|[5cc6bc9578312462...| 158|        Gloin|\n|[5cc6bc9578312462...| 195|       Thorin|\n|[5cc6bc9578312462...| 178|        Balin|\n|[5cc6bc9578312462...|  77|         Kili|\n|[5cc6bc9578312462...|1000|      Gandalf|\n|[5cc6bc9578312462...| 169|       Dwalin|\n|[5cc6bd1178312462...|1000|      Gandalf|\n|[5cc6bd1178312462...|  50|Bilbo Baggins|\n|[5cc6bd1178312462...| 178|        Balin|\n|[5cc6bd1178312462...|  77|         Kili|\n|[5cc6bd1178312462...| 167|          Oin|\n|[5cc6bd1178312462...| 158|        Gloin|\n|[5cc6bd1178312462...| 169|       Dwalin|\n|[5cc6bd1178312462...| 195|       Thorin|\n|[5cc6bd1178312462...|  82|         Fili|\n|[5cc6bd1178312462...|null|       Bombur|\n+--------------------+----+-------------+\nonly showing top 20 rows\n\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "\nprint(\"CHANGE COLLECTION TO people\")\n\ndf \u003d spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n    .option(\"uri\", \"mongodb://127.0.0.1/people.contacts\")\\\n    .load()\n\ndf.show()\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "AGGREGATION\n+--------------------+----+-------+\n|                 _id| age|   name|\n+--------------------+----+-------+\n|[5cc6ac7d78312462...| 195| Thorin|\n|[5cc6ac7d78312462...| 158|  Gloin|\n|[5cc6ac7d78312462...|1000|Gandalf|\n|[5cc6ac7d78312462...| 167|    Oin|\n|[5cc6ac7d78312462...| 178|  Balin|\n|[5cc6ac7d78312462...| 169| Dwalin|\n|[5cc6bc4c78312462...| 167|    Oin|\n|[5cc6bc4c78312462...|1000|Gandalf|\n|[5cc6bc4c78312462...| 169| Dwalin|\n|[5cc6bc4c78312462...| 195| Thorin|\n|[5cc6bc4c78312462...| 178|  Balin|\n|[5cc6bc4c78312462...| 158|  Gloin|\n+--------------------+----+-------+\n\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "\nprint(\"AGGREGATION\")\n\npipeline \u003d \"{\u0027$match\u0027: {\u0027age\u0027: { \u0027$gte\u0027: 150 }}}\"\ndf \u003d spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n    .option(\"pipeline\", pipeline)\\\n    .load()\n\ndf.show()\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "FILTER\n+--------------------+----+-------+\n|                 _id| age|   name|\n+--------------------+----+-------+\n|[5cc6ac7d78312462...|1000|Gandalf|\n|[5cc6bc4c78312462...|1000|Gandalf|\n+--------------------+----+-------+\n\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "\nprint(\"FILTER\")\ndf.filter(df[\u0027age\u0027] \u003e\u003d 1000)\\\n    .show()\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "SQL\n"
          ],
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-15-af5aa44a770c\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"temp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT name, age FROM temp WHERE name LIKE \u0027%Gandalf%\u0027\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 6\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \"\"\"\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 378\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value \u003d get_return_value(\n\u001b[0;32m-\u003e 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o291.showString.\n: java.lang.IllegalStateException: SparkContext has been shutdown\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2053)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3383)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
          ],
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o291.showString.\n: java.lang.IllegalStateException: SparkContext has been shutdown\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2053)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3383)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
          "output_type": "error"
        }
      ],
      "source": "\nprint(\"SQL\")\n\ndf.createOrReplaceTempView(\"temp\")\ndf \u003d spark.sql(\"SELECT name, age FROM temp WHERE name LIKE \u0027%Gandalf%\u0027\")\ndf.show()\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "source": "spark.stop()\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}